Feb 1 2023
Week 4: Day 1
    Questions to answer:    
        How does a random forest model make an inference for a given sample based on the decision derived by each tree in the forest?
        What are the hyperparameters associated with the random forest classifier and what is their purpose?
        How is a ROC curve created and how do you read it?
        What is AUC?
        What are some of the advantages and disadvantages of random forest?


        Features
        Gini
        Recall
        
        5.1
            Prepare a random forest for classification 
            Still feature-based supervised model
            classification returns a discrete value
            Regression returns a numerical value
            Random Forest Model:
                How random forest model makes inferences
                How confidence scores are used in a classfication result
            Main problem of decision tree is accuracy and precision.
            Benefits are its simple explainable performant and works well with small amount of data
            Intuition:
                if you use multiple models with moderate performacne, their combination delivers significantly improved overall performance
            Forest refers to multiple decision trees
            1. All the decision trees in the forest examine the sample, and makes an inference
            2. Based on the results, the trees vote for the class
                Confidence score is normalized score
            3. Because of the voting, results can be expressed as a confidence score ( not confidence is not the same as probability)
            hyperparameters (NOT ASSIGNED BY THE TRAINING PROCESS)
                numerical values associated with the model's structure or with the learning algorithm
                hyperparameters must be specified prior to training
                Two types: 
                    Model hyperparameters
                    Algorithm hyperparameters
            Key hyperparameters
                Number of esitmators
                    Number of decision trees
                Maximum depth
                    Max depth of deepest tree
                Sample per esitmator
                    Max samples used to create decision tree
                Features per split
                    Max number of features considered in each split
            Put it all together
            For each of “Number of Estimators” decision tree, do the following:
                1. Select “Samples Per Estimator” number of samples from the historical data

                2. Learn a decision tree of “Maximum depth” from those samples, but considering only “Features per split” number of features at each decision
        
        5.2

        5.3
            Reciever Operator Charactersitic curve or ROC curve
                ROC:
                    How do we pick confidence values?
                        We may want a higher confidence value if optimizing for precision, and a lower one if optimizing for recall
                        May want different confidence values for each class
                A tool for helping understand trade off between TP and FP
                Two measurements needed for ROC:
                    True positive Rate  = TP/ TP + FN
                    False Positive Rate = FP/ FP + TN

                AOC:
                    Area under curve can be used as a means to compare two ROC curves
                    Compares TPR(Y) vs. FPR(X)
            When to use random forest classifier:
                ADV:
                    Works well with little hyperparameter tuning
                    Does not overfit the data
                    Can be used for both regression and classification 
                DISADV:
                    Feature-based 
                        you have to have features coming in
                    Gradient-based approaches may outperform RF after tuned
                    Not ideal  for cases where there is a known model structure or causal relationships
                    Does not perform as well as other approaches when the number of features is high relative to the number of samples

        Discussion:
            ask, are random forest basically group of randomly generated decision trees based on the attributes of the original data 

Week 5: Day 1
    Questions to answer:
        Given a linear model, set of test samples, and ground truth, how do you calculate the mean squared error?
        How is the number of parameters related to the number of training samples in a linear regression model?
        What are some ways a linear regression model (with multiple features) is trained?
        What is the i.i.d. assumption and what machine learning approaches make that assumption?
        How can a classification model be used for regression?
    
    6.1
        Linear Regression
            f(x) = wx+b
            w and b are parameters - and are learned from historical data
            f(x1) = w0 + w1x1 ----------w stands for weight
            Training data is data where we already know the answer ( this is supervised)
        Samples
            each instance in training data
        Features
            the measurements or attributes associated with each sample
        Target
            Value we are trying to predict
        NOTATION:
            One feature: 
                f(x1) = w0 + w1x1 
                 ws are parameters
            Multiple Features
                f(x...xm) = w0 + sumi->m wixi
                w0....wm are parameters with m features
            Vector notation:
                fw(X) = WtX
            W,X are column vectors of size m+1, we will use lower case letters (xi,xi) to specify positions within those vectors x0 = 1
            m is the numeber of features, also called the dimension of the observation space
            
            We assume there are n samples in the training data
            X vector associated to each sample
            i.e. they are the features associated with each sample we often call Xi a sample
    6.2 
        response variable is the variable being modeled or predicted
            dependent variable or output or outcome
        predictor variable is the variable used to predict the response
            independent variable or input or covariate
        simple linear regression model is Y = B0+B1X+e, where e is the regression error term
        Regression error: e = Y - E(Y)
            E(Y) is expected value of Y
        Sum of absolute errors (SAE) is sum of distance from regression line absolute value of all errors added as opposed to them all added not absolute

    6.3

    6.4
        Error minimization:
            Calculate MSE
            independant and identically distributed (i.i.d)
                selected from same domain but not dependent on other choices
                the distribution is the same for all samples
            Key Questions:
                Need to find vector w such that fw(x) is as close to the test-set as possible
            MSE:
                Samples
                Functions:
                    looking for w in fw(x)
                n-sized ground truth vector Y
            MSE is computed as 1/n (sum n) (fw(Xi)-yi)^211
        Instead of classes use Values
            Many classifiers use parameter-based fitting decision tree and random forest do not, but can still be used for regression

    6.5
        Understand and interpret coefficient of determination R^2, mean squared error MSE , root mean squared error RMSE , and mean absolute error MAE.
        R^2 is the proportion of variation explained by the quadratic regression 
        MSE is heavily influenced by unusual values RMSE is not
    6.6

Week 5: Day 2
    Chapter 7
        Describe how optimization techniques are used in machine learning
    Describe various calculus techniques relevant to gradient descent
    Describe how a linear regression model can be trained by gradient descent
    Define regularization
    Define L1 and L2 penalties
    
    How do you compute a gradient vector for a given function?
    How does a local minima differ from a global minima? What types of functions have global and local minima?  What guarantees does gradient descent make about finding global and/or local minima?
    How does gradient descent adjust weights at each iteration?
    What are the hyper parameters of gradient descent and what is there meaning?
    What is overfitting? How does regularization help overcome the problem of overfitting?
    What is the difference between Ridge regression and Lasso regression? What types of penalties do they use?

    Calculus Review
        Function:
            Convex, any two points on the function would not lie below the function if a line is drawn between them
            all minima and maxima derivative will be 0
            Partial derivative:
                we take the derivatives for each feature and treat the other features as constant values
        Gradient Vector
            given a fuction that takes a vector as an argument
            you have a value for every feature in the gradient vector
        Regularization:
            lots of weights with small values is a sign of overfitting
        Gradient Descent may overfit because some of the features may not be useful

        M is the number of features
        L1 penalty:
            (hyperparameter)(sum of m)(|Weight|)
        L2 penalty:
            (hyperparameter)(sum of m)(W^2)
        Ridge Regression:
            uses L2 penalty
            shrinks coeffecients
            penalizes larger coefficients more due to squaring
        Lasso Regression:
            Uses L1 penalty
            selectively reduces coef
            Sometimes allows the designer to discard certain features(feature selection)
            Slower convergence
    Gradient Descent:
        Machine learning workhorse
        Convex function: can find global minima/maxima
        Non-convec: can find local
        Used in training in many ML approaches
        Algo:
            input f
            intitial vector of weights (w...)
            step size y
            convergence criteria e(epsilon)
                r = 0
                Whilel r = 0 or abs(Wr-Wr-1) > e
                    r=r+1
                    Wr = Wr-1 - y(gradient vector(Wr-1))
            **finding values that are getting closer and closer to when that derivative is 0
    Gradient gives direction of highest ascent
    Size of step is called learning rate

    Week 6 Day 1:
        Gradient boosted decision trees
            Feature-based
            Problem:
                Find a series of functions where if you add them up they will give you y-hat (value for target class)
                Where omega is the regularization function
            Cannot use gradient descent 
            We want an algo that will leark k functions to minimize loss and regularization

            Additive boosting
                At each round t we find a new learner to add to our ensemble
                yi(t-1) is prediction from previous round
            XGBoost
                from the obj function at t, an impurity function is derived
                impurity function is used to crete splits
        Goal of SVM is find a maximum margin hyperplane that keeps two classes as far apart as possible
        Kernel trick allos tranformation to linear relationships
        SVM uses a maximum margin hyperplane
